// Copyright 2019-present, the HuggingFace Inc. team, The Google AI Language Team and Facebook, Inc.
// Copyright 2019-2020 Guillaume Becquin
// Copyright 2020 Maarten van Gompel
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//! # Sequence classification pipeline (e.g. Sentiment Analysis)
//! More generic sequence classification pipeline, works with multiple models (Bert, Roberta)
//!
//! ```no_run
//! use rust_bert::pipelines::sequence_classification::SequenceClassificationConfig;
//! use rust_bert::resources::{RemoteResource};
//! use rust_bert::distilbert::{DistilBertModelResources, DistilBertVocabResources, DistilBertConfigResources};
//! use rust_bert::pipelines::sequence_classification::RerankingModel;
//! use rust_bert::pipelines::common::ModelType;
//! # fn main() -> anyhow::Result<()> {
//!
//! //Load a configuration
//! use rust_bert::pipelines::common::ModelResource;
//! let config = SequenceClassificationConfig::new(ModelType::DistilBert,
//!    ModelResource::Torch(Box::new(RemoteResource::from_pretrained(DistilBertModelResources::DISTIL_BERT_SST2))),
//!    RemoteResource::from_pretrained(DistilBertVocabResources::DISTIL_BERT_SST2),
//!    RemoteResource::from_pretrained(DistilBertConfigResources::DISTIL_BERT_SST2),
//!    None, // Merge resources
//!    true, //lowercase
//!    None, //strip_accents
//!    None, //add_prefix_space
//! );
//!
//! //Create the model
//! let sequence_classification_model = RerankingModel::new(config)?;
//!
//! let input = [
//!     "Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it's not preachy or boring.",
//!     "This film tried to be too many things all at once: stinging political satire, Hollywood blockbuster, sappy romantic comedy, family values promo...",
//!     "If you like original gut wrenching laughter you will like this movie. If you are young or old then you will love this movie, hell even my mom liked it.",
//! ];
//! let output = sequence_classification_model.predict(&input);
//! # Ok(())
//! # }
//! ```
//! (Example courtesy of [IMDb](http://www.imdb.com))
//!
//! Output: \
//! ```no_run
//! # use rust_bert::pipelines::sequence_classification::Label;
//! let output =
//! [
//!    Label { text: String::from("POSITIVE"), score: 0.9986, id: 1, sentence: 0},
//!    Label { text: String::from("NEGATIVE"), score: 0.9985, id: 0, sentence: 1},
//!    Label { text: String::from("POSITIVE"), score: 0.9988, id: 1, sentence: 12},
//! ]
//! # ;
//! ```
use rust_bert::RustBertError;
use rust_bert::pipelines::common::{
    ModelResource, ConfigOption, TokenizerOption,
};
use rust_bert::pipelines::sequence_classification::{SequenceClassificationOption,SequenceClassificationConfig};
use rust_tokenizers::tokenizer::TruncationStrategy;
use rust_tokenizers::TokenizedInput;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tch::{no_grad, Device, Kind, Tensor, TchError};

#[derive(Debug, Serialize, Deserialize, Clone)]
/// # Label generated by a `RerankingModel`
pub struct RankedResult {
    /// Label String representation
    pub text: String,
    /// Confidence score
    pub score: f64,
    /// Label ID
    pub rank: usize
}

#[derive(Debug, Serialize, Deserialize, Clone)]
/// # Label generated by a `RerankingModel`
pub struct RankedResults {
    /// queries vector
    pub queries: Vec<String>,
    ///ranked results vector corresponding with queries vector
    pub ranked_results: Vec<Vec<RankedResult>>
}

/// # Configuration for RerankingModel
/// Contains information regarding the model to load and device to place the model on.

/// # RerankingModel for Classification (e.g. Sentiment Analysis)
pub struct RerankingModel {
    tokenizer: TokenizerOption,
    sequence_classifier: SequenceClassificationOption,
    label_mapping: HashMap<i64, String>,
    device: Device,
    max_length: usize,
}

fn get_device(_model_resource: ModelResource, device: Device) -> Device {
    #[cfg(feature = "onnx")]
    let device = if let ModelResource::ONNX(_) = _model_resource {
        Device::Cpu
    } else {
        device
    };

    #[cfg(not(feature = "onnx"))]
    let device = device;
    device
}

impl RerankingModel {
    /// Build a new `RerankingModel`
    ///
    /// # Arguments
    ///
    /// * `config` - `SequenceClassificationConfig` object containing the resource references (model, vocabulary, configuration) and device placement (CPU/GPU)
    ///
    /// # Example
    ///
    /// ```no_run
    /// # fn main() -> anyhow::Result<()> {
    /// use rust_bert::pipelines::sequence_classification::RerankingModel;
    ///
    /// let model = RerankingModel::new(Default::default())?;
    /// # Ok(())
    /// # }
    /// ```
    pub fn new(
        config: SequenceClassificationConfig,
    ) -> Result<RerankingModel, RustBertError> {
        let vocab_path = config.vocab_resource.get_local_path()?;
        let merges_path = config
            .merges_resource
            .as_ref()
            .map(|resource| resource.get_local_path())
            .transpose()?;

        let tokenizer = TokenizerOption::from_file(
            config.model_type,
            vocab_path.to_str().unwrap(),
            merges_path.as_deref().map(|path| path.to_str().unwrap()),
            config.lower_case,
            config.strip_accents,
            config.add_prefix_space,
        )?;
        Self::new_with_tokenizer(config, tokenizer)
    }

    /// Build a new `RerankingModel` with a provided tokenizer.
    ///
    /// # Arguments
    ///
    /// * `config` - `SequenceClassificationConfig` object containing the resource references (model, vocabulary, configuration) and device placement (CPU/GPU)
    /// * `tokenizer` - `TokenizerOption` tokenizer to use for sequence classification.
    ///
    /// # Example
    ///
    /// ```no_run
    /// # fn main() -> anyhow::Result<()> {
    /// use rust_bert::pipelines::common::{ModelType, TokenizerOption};
    /// use rust_bert::pipelines::sequence_classification::RerankingModel;
    /// let tokenizer = TokenizerOption::from_file(
    ///     ModelType::Bert,
    ///     "path/to/vocab.txt",
    ///     None,
    ///     false,
    ///     None,
    ///     None,
    /// )?;
    /// let model = RerankingModel::new_with_tokenizer(Default::default(), tokenizer)?;
    /// # Ok(())
    /// # }
    /// ```
    pub fn new_with_tokenizer(
        config: SequenceClassificationConfig,
        tokenizer: TokenizerOption,
    ) -> Result<RerankingModel, RustBertError> {
        let config_path = config.config_resource.get_local_path()?;
        let sequence_classifier = SequenceClassificationOption::new(&config)?;

        let model_config = ConfigOption::from_file(config.model_type, config_path);
        let max_length = model_config
            .get_max_len()
            .map(|v| v as usize)
            .unwrap_or(usize::MAX);
        let label_mapping = model_config.get_label_mapping().clone();
        let device = get_device(config.model_resource, config.device);
        Ok(RerankingModel {
            tokenizer,
            sequence_classifier,
            label_mapping,
            device,
            max_length,
        })
    }

    /// Get a reference to the model tokenizer.
    pub fn get_tokenizer(&self) -> &TokenizerOption {
        &self.tokenizer
    }

    /// Get a mutable reference to the model tokenizer.
    pub fn get_tokenizer_mut(&mut self) -> &mut TokenizerOption {
        &mut self.tokenizer
    }
    // Get a reference to the max_sequence_length from the model's config.
    pub fn get_max_len(&self) -> &usize {
        &self.max_length
    }

    fn prepare_for_model<'a, S ,V>(&self, queries: S, results_set: V) -> Vec<(Vec<&'a str>,Tensor,Tensor)>
    where 
        S: AsRef<[&'a str]>,
        V: AsRef<[Vec<&'a str>]>
    {
        let text_pair_lists: Vec<Vec<(&'a str,&'a str)>> = queries
            .as_ref()
            .iter()
            .zip(results_set.as_ref().iter())
            .map(|(query,results)| {
                results
                    .iter()
                    .map(|result| {
                        (*query,*result)
                    })
                .collect::<Vec<(&'a str,&'a str)>>()
        }).collect::<Vec<Vec<(&'a str,&'a str)>>>();
        text_pair_lists
            .iter()
            .map(|text_pair_list| {
                let mut tokenized_input = self.tokenizer.encode_pair_list(
                    text_pair_list,
                    self.max_length,
                    &TruncationStrategy::LongestFirst,
                    0
                );
                let max_len = match tokenized_input
                    .iter()
                    .map(|input| input.token_ids.len())
                    .max() 
                {
                    Some(index) => index,
                    None => self.max_length+1
                };
                
                let pad_id = self
                    .tokenizer
                    .get_pad_id()
                    .expect("The Tokenizer used for sequence classification should contain a PAD id");
                let tokenized_input_tensors: Vec<Tensor> = tokenized_input
                    .iter_mut()
                    .map(|input| {
                        input.token_ids.resize(max_len, pad_id);
                        Tensor::from_slice(&(input.token_ids))
                    })
                .collect::<Vec<_>>();
                let token_type_ids: Vec<Tensor> = tokenized_input
                    .iter_mut()
                    .map(|input| {
                        input
                            .segment_ids
                            .resize(max_len, *input.segment_ids.last().unwrap_or(&0));
                        Tensor::from_slice(&(input.segment_ids))
                    })
                .collect::<Vec<_>>();
                (
                    text_pair_list.iter().map(|pair| pair.1).collect::<Vec<&'a str>>(),
                    Tensor::stack(tokenized_input_tensors.as_slice(), 0).to(self.device),
                    Tensor::stack(token_type_ids.as_slice(), 0)
                        .to(self.device)
                        .to_kind(Kind::Int64)
                )
            })
        .collect::<Vec<(Vec<&'a str>,Tensor,Tensor)>>()
    }
    /// Classify texts
    ///
    /// # Arguments
    ///
    /// * `input` - `&[&str]` Array of texts to classify.
    ///
    /// # Returns
    ///
    /// * `Vec<Label>` containing labels for input texts
    ///
    /// # Example
    ///
    /// ```no_run
    /// # fn main() -> anyhow::Result<()> {
    /// # use rust_bert::pipelines::sequence_classification::RerankingModel;
    ///
    /// let sequence_classification_model =  RerankingModel::new(Default::default())?;
    /// let input = [
    ///     "Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it's not preachy or boring.",
    ///     "This film tried to be too many things all at once: stinging political satire, Hollywood blockbuster, sappy romantic comedy, family values promo...",
    ///     "If you like original gut wrenching laughter you will like this movie. If you are young or old then you will love this movie, hell even my mom liked it.",
    /// ];
    /// let output = sequence_classification_model.predict(&input);
    /// # Ok(())
    /// # }
    /// ```
    pub fn predict<'a, S, V>(&self, queries: S, results_set: V, logit_index_threshold: i64) -> Result<RankedResults,RustBertError>
    where
        S: AsRef<[&'a str]>,
        V: AsRef<[Vec<&'a str>]>
    {
        let model_inputs: Vec<(Vec<&str>, Tensor, Tensor)> = self.prepare_for_model(queries.as_ref(),results_set.as_ref());
        let ranked_results_vector = model_inputs
            .iter()
            .map(|(results,input_ids,token_type_ids)| {
                let probs = no_grad(|| {
                    let logits: Tensor = self.sequence_classifier.forward_t(
                        Some(input_ids),
                        None,
                        Some(token_type_ids),
                        None,
                        None,
                        false,
                    );
                    let softmax_probs = logits.softmax(-1, Kind::Float);
                    softmax_probs.slice(-1, 0, logit_index_threshold, 1)   
                });
                println!("Probs:\n{:?}",probs);
                let dims: Vec<i64> = vec![-1];
                let prob_scores: Tensor = match probs.f_sum_dim_intlist(&dims, true, Kind::Float) {
                    Ok(scores) => scores.detach().to(Device::Cpu).view(-1),
                    Err(tch_err) => {
                        println!("Failed on sum\n{:?}",tch_err);
                        Tensor::empty(&[0],(Kind::Int64,Device::Cpu))
                    }
                };
                let rankings_index_map: Tensor = prob_scores.as_ref().argsort(0,true);
                println!("Rankings:\n{:?}",rankings_index_map);
                println!("Results:\n{:?}",results);
                match rankings_index_map.iter::<i64>() {
                    Ok(iter) => {
                        iter
                            .zip(results.iter())
                            .enumerate()
                            .map(|(rank,(i,res))| {
                                RankedResult {
                                    text:String::from(*res),
                                    score:prob_scores.double_value(&[i]),
                                    rank:rank+1
                                }
                            })
                        .collect::<Vec<RankedResult>>()
                    },
                    Err(rust_bert_err) => {
                        println!("Failed rustbert\n{:?}",rust_bert_err);
                        vec![]
                    }
                }
                    
            })
        .collect::<Vec<Vec<RankedResult>>>();
        Ok(RankedResults {
            queries: queries.as_ref().iter().map(|q| String::from(*q)).collect::<Vec<String>>(),
            ranked_results: ranked_results_vector
        })
    }
}