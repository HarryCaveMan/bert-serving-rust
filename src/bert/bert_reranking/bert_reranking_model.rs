// Copyright 2019-present, the HuggingFace Inc. team, The Google AI Language Team and Facebook, Inc.
// Copyright 2019-2020 Guillaume Becquin
// Copyright 2020 Maarten van Gompel
// Copyright 2024 Harris Joseph
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
//
//
//
// # query/result reranking (via n-label paired sequence classification)
// More generic sequence classification pipeline, works with multiple models (Bert, Roberta)
// Private module used mostly by the bert_reranking::builder module

use rust_bert::RustBertError;
use rust_bert::pipelines::common::{
    ModelResource, ConfigOption, TokenizerOption,
};
use rust_bert::pipelines::sequence_classification::{SequenceClassificationOption,SequenceClassificationConfig};
use rust_tokenizers::tokenizer::TruncationStrategy;
use serde::{Deserialize, Serialize};
use tch::{no_grad, Device, Kind, Tensor};

#[derive(Debug, Serialize, Deserialize, Clone)]
/// # LRankedResultabel generated by a `RerankingModel`
pub struct RankedResult {
    /// Result text
    pub text: String,
    /// Confidence score
    pub score: f64,
    /// Rank in results by score
    pub rank: usize
}

#[derive(Debug, Serialize, Deserialize, Clone)]
/// # RankedResults generated by a `RerankingModel`
/// This struct is the only one in this (bert_reranking_model) module to be exposed publicly 
/// outside the bert_reranking module (via pub use)
pub struct RankedResults {
    /// Queries vector
    pub queries: Vec<String>,
    /// Ranked results vector corresponding with queries vector
    pub ranked_results: Vec<Vec<RankedResult>>
}

/// # Configuration for RerankingModel
/// Contains information regarding the model to load and device to place the model on.

/// # RerankingModel for query/results pairs (via pair-encoded n-label sequence classification)
pub struct RerankingModel {
    tokenizer: TokenizerOption,
    sequence_classifier: SequenceClassificationOption,
    device: Device,
    max_length: usize,
}

fn get_device(_model_resource: ModelResource, device: Device) -> Device {
    #[cfg(feature = "onnx")]
    let device = if let ModelResource::ONNX(_) = _model_resource {
        Device::Cpu
    } else {
        device
    };

    #[cfg(not(feature = "onnx"))]
    let device = device;
    device
}

impl RerankingModel {
    /// Build a new `RerankingModel`
    ///
    /// # Arguments
    ///
    /// * `config` - `SequenceClassificationConfig` object containing the resource references (model, vocabulary, configuration) and device placement (CPU/GPU)
    ///
    /// Private struct impl used mostly by super::builder 
    /// 
    pub fn new(
        config: SequenceClassificationConfig,
    ) -> Result<RerankingModel, RustBertError> {
        let vocab_path = config.vocab_resource.get_local_path()?;
        let merges_path = config
            .merges_resource
            .as_ref()
            .map(|resource| resource.get_local_path())
            .transpose()?;

        let tokenizer = TokenizerOption::from_file(
            config.model_type,
            vocab_path.to_str().unwrap(),
            merges_path.as_deref().map(|path| path.to_str().unwrap()),
            config.lower_case,
            config.strip_accents,
            config.add_prefix_space,
        )?;
        Self::new_with_tokenizer(config, tokenizer)
    }

    /// Build a new `RerankingModel` with a provided tokenizer.
    ///
    /// # Arguments
    ///
    /// * `config` - `SequenceClassificationConfig` object containing the resource references (model, vocabulary, configuration) and device placement (CPU/GPU)
    /// * `tokenizer` - `TokenizerOption` tokenizer to use for sequence classification.
    ///
    /// 
    pub fn new_with_tokenizer(
        config: SequenceClassificationConfig,
        tokenizer: TokenizerOption,
    ) -> Result<RerankingModel, RustBertError> {
        let config_path = config.config_resource.get_local_path()?;
        let sequence_classifier = SequenceClassificationOption::new(&config)?;

        let model_config = ConfigOption::from_file(config.model_type, config_path);
        let max_length = model_config
            .get_max_len()
            .map(|v| v as usize)
            .unwrap_or(usize::MAX);
        let device = get_device(config.model_resource, config.device);
        Ok(RerankingModel {
            tokenizer,
            sequence_classifier,
            device,
            max_length,
        })
    }

    /// Get a reference to the model tokenizer.
    pub fn get_tokenizer(&self) -> &TokenizerOption {
        &self.tokenizer
    }

    /// Get a mutable reference to the model tokenizer.
    pub fn get_tokenizer_mut(&mut self) -> &mut TokenizerOption {
        &mut self.tokenizer
    }
    // Get a reference to the max_sequence_length from the model's config.
    pub fn get_max_len(&self) -> &usize {
        &self.max_length
    }
    /// Build a new `RerankingModel` with a provided tokenizer.
    ///
    /// # Arguments
    ///
    /// * `queries` - `S: AsRef<[&'a str]>` object containing the queries vector
    /// * `results_set` - `V: AsRef<[Vec<&'a str>]>` object containing 2d vector of result vectors for each wuery
    ///
    /// # Returns
    ///
    /// `Vec<(Vec<&'a str>,Tensor,Tensor)>` containing results and model inputs for each query
    ///
    fn prepare_for_model<'a, S ,V>(&self, queries: S, results_set: V) -> Vec<(Vec<&'a str>,Tensor,Tensor)>
    where 
        S: AsRef<[&'a str]>,
        V: AsRef<[Vec<&'a str>]>
    {
        let text_pair_lists: Vec<Vec<(&'a str,&'a str)>> = queries
            .as_ref()
            .iter()
            .zip(results_set.as_ref().iter())
            .map(|(query,results)| {
                results
                    .iter()
                    .map(|result| {
                        (*query,*result)
                    })
                .collect::<Vec<(&'a str,&'a str)>>()
        }).collect::<Vec<Vec<(&'a str,&'a str)>>>();
        text_pair_lists
            .iter()
            .map(|text_pair_list| {
                let mut tokenized_input = self.tokenizer.encode_pair_list(
                    text_pair_list,
                    self.max_length,
                    &TruncationStrategy::LongestFirst,
                    0
                );
                let max_len = match tokenized_input
                    .iter()
                    .map(|input| input.token_ids.len())
                    .max() 
                {
                    Some(index) => index,
                    None => self.max_length+1
                };
                
                let pad_id = self
                    .tokenizer
                    .get_pad_id()
                    .expect("The Tokenizer used for sequence classification should contain a PAD id");
                let tokenized_input_tensors: Vec<Tensor> = tokenized_input
                    .iter_mut()
                    .map(|input| {
                        input.token_ids.resize(max_len, pad_id);
                        Tensor::from_slice(&(input.token_ids))
                    })
                .collect::<Vec<_>>();
                let token_type_ids: Vec<Tensor> = tokenized_input
                    .iter_mut()
                    .map(|input| {
                        input
                            .segment_ids
                            .resize(max_len, *input.segment_ids.last().unwrap_or(&0));
                        Tensor::from_slice(&(input.segment_ids))
                    })
                .collect::<Vec<_>>();
                (
                    text_pair_list.iter().map(|pair| pair.1).collect::<Vec<&'a str>>(),
                    Tensor::stack(tokenized_input_tensors.as_slice(), 0).to(self.device),
                    Tensor::stack(token_type_ids.as_slice(), 0)
                        .to(self.device)
                        .to_kind(Kind::Int64)
                )
            })
        .collect::<Vec<(Vec<&'a str>,Tensor,Tensor)>>()
    }
    /// Classify texts
    ///
    /// # Arguments
    ///
    /// * `queries` - `S: AsRef<[&'a str]>` Array containing the queries vector
    /// * `results_set` - `V: AsRef<[Vec<&'a str>]>`  2d array of result vectors(arrays) for each query
    ///
    /// # Returns
    ///
    /// * `RankedResults` containing results for all query/results pairs
    ///
    pub fn predict<'a, S, V>(&self, queries: S, results_set: V, logit_index_threshold: i64) -> RankedResults
    where
        S: AsRef<[&'a str]>,
        V: AsRef<[Vec<&'a str>]>
    {
        let model_inputs: Vec<(Vec<&str>, Tensor, Tensor)> = self.prepare_for_model(queries.as_ref(),results_set.as_ref());
        let ranked_results_vector = model_inputs
            .iter()
            .map(|(results,input_ids,token_type_ids)| {
                let probs = no_grad(|| {
                    let logits: Tensor = self.sequence_classifier.forward_t(
                        Some(input_ids),
                        None,
                        Some(token_type_ids),
                        None,
                        None,
                        false,
                    );
                    let softmax_probs = logits.softmax(-1, Kind::Float);
                    softmax_probs.slice(-1, 0, logit_index_threshold, 1)   
                });
                let dims: Vec<i64> = vec![-1];
                let prob_scores: Tensor = match probs.f_sum_dim_intlist(&dims, true, Kind::Float) {
                    Ok(scores) => scores.detach().to(Device::Cpu).view(-1),
                    Err(tch_err) => {
                        println!("Failed on sum\n{:?}",tch_err);
                        Tensor::empty(&[0],(Kind::Int64,Device::Cpu))
                    }
                };
                let rankings_index_map: Tensor = prob_scores.as_ref().argsort(0,true);
                match rankings_index_map.iter::<i64>() {
                    Ok(iter) => {
                        iter
                            .zip(results.iter())
                            .enumerate()
                            .map(|(rank,(i,res))| {
                                RankedResult {
                                    text:String::from(*res),
                                    score:prob_scores.double_value(&[i]),
                                    rank:rank+1
                                }
                            })
                        .collect::<Vec<RankedResult>>()
                    },
                    Err(rust_bert_err) => {
                        println!("Failed rustbert\n{:?}",rust_bert_err);
                        vec![]
                    }
                }
                    
            })
        .collect::<Vec<Vec<RankedResult>>>();
        RankedResults {
            queries: queries.as_ref().iter().map(|q| String::from(*q)).collect::<Vec<String>>(),
            ranked_results: ranked_results_vector
        }
    }
}